{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.1. Importing all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.2. Setting some global settins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.3. Importing the data file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clp_network(trial=False, n_l=-1, units=[]):\n",
    "    custom_network = not trial and len(units) == n_l\n",
    "    if custom_network: n_layers = n_l\n",
    "    else: n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 6\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        if custom_network: out_features = units[i]\n",
    "        else: out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 10)\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, 2))\n",
    "    layers.append(nn.LeakyReLU())\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Making a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clp_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.labels = [label for label in df['loyalty']]\n",
    "        self.features = df.drop(columns=['loyalty'], axis=1).values.tolist()\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_features(self, idx):\n",
    "        return np.array(self.features[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_features = self.get_batch_features(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_features, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Training and evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(param, model, trial=False):\n",
    "\n",
    "    # extracting parameters\n",
    "    if 'n_epochs' in param: n_epochs = param['n_epochs']\n",
    "    else: n_epochs = 50 # DEFAULT VALUE\n",
    "\n",
    "    if 'batch_size' in param: batch_size = param['batch_size']\n",
    "    else: batch_size = 32 # DEFAULT VALUE\n",
    "\n",
    "    # if 'units' in param: units = param['units']\n",
    "    # else: units = []\n",
    "\n",
    "    # if 'n_layers' in param: n_layers = param['n_layers']\n",
    "    # else: n_layers = -1\n",
    "    # # checking if we're using a custom model\n",
    "    # custom_model = not trial and len(units) == n_layers\n",
    "    \n",
    "    # importing data from file\n",
    "    data_path = os.path.join(os.getcwd(), 'Data\\\\processed_customer_data.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data.drop(columns='ID', inplace=True)\n",
    "    \n",
    "    train_data, val_data = tts(data, test_size = 0.2)\n",
    "    train, val = clp_dataset(train_data), clp_dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(n_epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in train_dataloader:\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                train_input = train_input.to(device)\n",
    "\n",
    "                output = model(train_input.float())\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_input = val_input.to(device)\n",
    "\n",
    "                    output = model(val_input.float())\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            accuracy = total_acc_val/len(val_data)\n",
    "            \n",
    "            # Add prune mechanism\n",
    "            if trial:\n",
    "                trial.report(accuracy, epoch_num)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4. Making the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, n_epochs, batch_size):\n",
    "     ### ADD OTHER PARAMS\n",
    "     params = {\n",
    "              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "              'n_epochs': n_epochs,\n",
    "              'batch_size': batch_size\n",
    "              }\n",
    "     model = clp_network(trial)\n",
    "     accuracy, _ = train_and_evaluate(params, model, trial)\n",
    "     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-30 12:45:05,988]\u001b[0m A new study created in memory with name: no-name-025e9ac5-0011-4937-a1ac-054c8e86416d\u001b[0m\n",
      "\u001b[32m[I 2022-08-30 12:45:12,156]\u001b[0m Trial 0 finished with value: 0.7214285714285714 and parameters: {'learning_rate': 0.00010637794544522072, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 7, 'n_units_l1': 4}. Best is trial 0 with value: 0.7214285714285714.\u001b[0m\n",
      "\u001b[32m[I 2022-08-30 12:45:12,703]\u001b[0m Trial 1 finished with value: 0.7571428571428571 and parameters: {'learning_rate': 4.395858286933057e-05, 'optimizer': 'SGD', 'n_layers': 4, 'n_units_l0': 5, 'n_units_l1': 8, 'n_units_l2': 5, 'n_units_l3': 7}. Best is trial 1 with value: 0.7571428571428571.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study:    n_epoch = 10 batch_size = 64\n",
      "accuracy: 0.7571428571428571\n",
      "Study 0:    n_epoch = 10 batch_size = 64\n",
      "accuracy: 0.7571428571428571\n",
      "learning_rate 4.395858286933057e-05\n",
      "optimizer SGD\n",
      "n_layers 4\n",
      "n_units_l0 5\n",
      "n_units_l1 8\n",
      "n_units_l2 5\n",
      "n_units_l3 7\n"
     ]
    }
   ],
   "source": [
    "bests = []\n",
    "\n",
    "# for e in range(30,151,30): ########################################################\n",
    "for e in range(10,11,100):\n",
    "    # for b in [4, 8, 16, 32, 64]: ###################################################\n",
    "    for b in [64]:\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "        # study.optimize(lambda trial: objective(trial, e, b), n_trials=20) ############################################\n",
    "        study.optimize(lambda trial: objective(trial, e, b), n_trials=2)\n",
    "        best_params = study.best_trial\n",
    "        param_dict = best_params.params\n",
    "        print(f\"Study:    n_epoch = {e} batch_size = {b}\")\n",
    "        print(f\"accuracy: {best_params.value}\")\n",
    "\n",
    "        params_dict = dict()\n",
    "        params_dict['params'] = param_dict\n",
    "        params_dict['value'] = best_params.value\n",
    "        params_dict['n_epoch'] = e\n",
    "        params_dict['batch_size'] = b\n",
    "        bests.append(params_dict)\n",
    "\n",
    "for i in range(len(bests)):\n",
    "    param_dict = bests[i]['params']\n",
    "    print(\"Study \" + str(i) + \":    n_epoch = \" + str(bests[i]['n_epoch']) + \" batch_size = \" + str(bests[i]['batch_size']))\n",
    "    print('accuracy: ' + str(bests[i]['value']))\n",
    "    for k, v in param_dict.items():\n",
    "        print(k, end=' ')\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clp_network(trial=False, n_l=-1, n_u=[]):\n",
    "# params_dict = dict()\n",
    "# params_dict['params'] = param_dict\n",
    "# params_dict['value'] = best_params.value\n",
    "# params_dict['n_epoch'] = e\n",
    "# params_dict['batch_size'] = b\n",
    "# bests.append(params_dict)\n",
    "\n",
    "max_index = 0\n",
    "for i in range(1, len(bests)):\n",
    "    if bests[i]['value'] > bests[max_index]['value']: max_index = i\n",
    "\n",
    "best_case = bests[max_index]\n",
    "best_accuracy = best_case['value']\n",
    "best_n_epochs = best_case['n_epoch']\n",
    "best_batch_size = best_case['batch_size']\n",
    "\n",
    "best_params = best_case['params']\n",
    "\n",
    "best_lr = best_params['learning_rate']\n",
    "best_optimizer = best_params['optimizer']\n",
    "best_n_layers = best_params['n_layers']\n",
    "\n",
    "best_n_units = []\n",
    "for i in range(best_n_layers):\n",
    "    best_n_units.append(best_params[\"n_units_l{}\".format(i)])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('CLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fd8e93d66050a205cb7f64389af9f13c9c81ade6ed89497904bd8d3e012a9c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

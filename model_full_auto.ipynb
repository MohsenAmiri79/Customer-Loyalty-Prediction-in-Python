{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.1. Importing all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.2. Setting some global settins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.3. Importing the data file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clp_network(trial=False, n_layers=-1, units=[]):\n",
    "    \n",
    "    # checking if the network is custom or automatic\n",
    "    custom_network = not trial and len(units) == n_layers\n",
    "\n",
    "    # setting automatic number of layers\n",
    "    if not custom_network: n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
    "    \n",
    "    layers = []\n",
    "    in_features = 6\n",
    "    \n",
    "    # for each layer of the network\n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        # setting the number of units in the layer\n",
    "        if custom_network: out_features = units[i]\n",
    "        else: out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 10)\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, 2))\n",
    "    layers.append(nn.LeakyReLU())\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Making a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clp_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.labels = [label for label in df['loyalty']]\n",
    "        self.features = df.drop(columns=['loyalty'], axis=1).values.tolist()\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_features(self, idx):\n",
    "        return np.array(self.features[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_features = self.get_batch_features(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_features, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Training and evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(param, model, trial=False):\n",
    "\n",
    "    # extracting parameters\n",
    "    if 'n_epochs' in param: n_epochs = param['n_epochs']\n",
    "    else: n_epochs = 50 # DEFAULT VALUE\n",
    "\n",
    "    if 'batch_size' in param: batch_size = param['batch_size']\n",
    "    else: batch_size = 32 # DEFAULT VALUE\n",
    "    \n",
    "    # importing data from file\n",
    "    data_path = os.path.join(os.getcwd(), 'Data\\\\processed_customer_data.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data.drop(columns='ID', inplace=True)\n",
    "    \n",
    "    # separating train and dev sets\n",
    "    train_data, val_data = tts(data, test_size = 0.2)\n",
    "    train, val = clp_dataset(train_data), clp_dataset(val_data)\n",
    "\n",
    "    # creating data loaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    # activating CUDA\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # setting up the optimizer and evaluator objects\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "\n",
    "    # moving process to CUDA GPU\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    # epochs\n",
    "    for epoch_num in range(n_epochs):\n",
    "\n",
    "            # accuracy and loss of the epoch\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            # one training step\n",
    "            for train_input, train_label in train_dataloader:\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                train_input = train_input.to(device)\n",
    "\n",
    "                output = model(train_input.float())\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # repeating everything for the dev set\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_input = val_input.to(device)\n",
    "\n",
    "                    output = model(val_input.float())\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            # calculating the accuract of the model\n",
    "            accuracy = total_acc_val/len(val_data)\n",
    "            \n",
    "            # adding pruning mechanism\n",
    "            if trial:\n",
    "                trial.report(accuracy, epoch_num)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4. Making the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing models variable\n",
    "models = dict()\n",
    "\n",
    "# defining objective function of optuna study\n",
    "def objective(trial, n_epochs=False, batch_size=False):\n",
    "     \n",
    "     if not n_epochs: n_epochs = trial.suggest_int(\"n_epochs\", 30, 180)\n",
    "     if not batch_size: batch_size = 2**trial.suggest_int(\"batch_size\", 1, 6)\n",
    "     \n",
    "     # setting trial parameters\n",
    "     params = {\n",
    "              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-6, 1e-1),\n",
    "              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "              'n_epochs': n_epochs,\n",
    "              'batch_size': batch_size\n",
    "              }\n",
    "     # creating the model\n",
    "     model = clp_network(trial)\n",
    "\n",
    "     # training the model\n",
    "     accuracy, model = train_and_evaluate(params, model, trial)\n",
    "\n",
    "     # saving the model (if it has a higher accuract than the previous ones)\n",
    "     if accuracy > models['accuracy']:\n",
    "          models['accuracy'] = accuracy\n",
    "          models['model'] = model\n",
    "     \n",
    "     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-31 15:34:07,829]\u001b[0m A new study created in memory with name: no-name-c560c107-eaa9-454d-96cd-cb9d002c210a\u001b[0m\n",
      "c:\\Users\\Mohsen\\anaconda3\\envs\\CLP\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969a54aaaa2746678b8ba434f3af52fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-31 15:34:40,645]\u001b[0m Trial 0 finished with value: 0.7071428571428572 and parameters: {'n_epochs': 114, 'batch_size': 3, 'learning_rate': 0.0005431859416185268, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 7, 'n_units_l1': 6}. Best is trial 0 with value: 0.7071428571428572.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:37:53,578]\u001b[0m Trial 1 finished with value: 0.7357142857142858 and parameters: {'n_epochs': 116, 'batch_size': 1, 'learning_rate': 0.007308251427623791, 'optimizer': 'RMSprop', 'n_layers': 6, 'n_units_l0': 4, 'n_units_l1': 6, 'n_units_l2': 9, 'n_units_l3': 5, 'n_units_l4': 7, 'n_units_l5': 8}. Best is trial 1 with value: 0.7357142857142858.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:37:59,558]\u001b[0m Trial 2 finished with value: 0.7142857142857143 and parameters: {'n_epochs': 125, 'batch_size': 6, 'learning_rate': 1.1106687209243363e-05, 'optimizer': 'SGD', 'n_layers': 5, 'n_units_l0': 4, 'n_units_l1': 5, 'n_units_l2': 10, 'n_units_l3': 6, 'n_units_l4': 5}. Best is trial 1 with value: 0.7357142857142858.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:38:07,341]\u001b[0m Trial 3 finished with value: 0.75 and parameters: {'n_epochs': 49, 'batch_size': 4, 'learning_rate': 0.0003360534269642881, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 6}. Best is trial 3 with value: 0.75.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:38:14,056]\u001b[0m Trial 4 finished with value: 0.7714285714285715 and parameters: {'n_epochs': 54, 'batch_size': 4, 'learning_rate': 0.011826148286967032, 'optimizer': 'SGD', 'n_layers': 3, 'n_units_l0': 9, 'n_units_l1': 5, 'n_units_l2': 7}. Best is trial 4 with value: 0.7714285714285715.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:38:28,419]\u001b[0m Trial 5 finished with value: 0.7785714285714286 and parameters: {'n_epochs': 177, 'batch_size': 6, 'learning_rate': 0.0038476538368027325, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 8, 'n_units_l1': 10, 'n_units_l2': 8, 'n_units_l3': 9, 'n_units_l4': 8, 'n_units_l5': 7}. Best is trial 5 with value: 0.7785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:38:31,825]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:38:43,064]\u001b[0m Trial 7 finished with value: 0.7214285714285714 and parameters: {'n_epochs': 30, 'batch_size': 3, 'learning_rate': 0.0415394014930678, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 7, 'n_units_l1': 4, 'n_units_l2': 5, 'n_units_l3': 5}. Best is trial 5 with value: 0.7785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:38:44,917]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:40:11,457]\u001b[0m Trial 9 finished with value: 0.8571428571428571 and parameters: {'n_epochs': 57, 'batch_size': 1, 'learning_rate': 0.0009982858103527793, 'optimizer': 'Adam', 'n_layers': 3, 'n_units_l0': 5, 'n_units_l1': 7, 'n_units_l2': 8}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:40:16,480]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:04,196]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:12,394]\u001b[0m Trial 12 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 106, 'batch_size': 5, 'learning_rate': 4.6753162264804496e-05, 'optimizer': 'SGD', 'n_layers': 4, 'n_units_l0': 6, 'n_units_l1': 5, 'n_units_l2': 6, 'n_units_l3': 4}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:13,305]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:13,413]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:13,724]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:18,294]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:22,291]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:22,852]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:24,593]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:24,760]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:27,088]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:42,463]\u001b[0m Trial 22 finished with value: 0.8071428571428572 and parameters: {'n_epochs': 112, 'batch_size': 5, 'learning_rate': 0.009009574907403579, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 6, 'n_units_l1': 7, 'n_units_l2': 4, 'n_units_l3': 7, 'n_units_l4': 8}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:42,774]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:41:43,106]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:42:11,522]\u001b[0m Trial 25 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 110, 'batch_size': 3, 'learning_rate': 0.049222578232424916, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 6, 'n_units_l1': 7}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:42:12,037]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:42:13,288]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:19,049]\u001b[0m Trial 28 finished with value: 0.7642857142857142 and parameters: {'n_epochs': 125, 'batch_size': 3, 'learning_rate': 0.0003830858932664436, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 7, 'n_units_l1': 10, 'n_units_l2': 10, 'n_units_l3': 5, 'n_units_l4': 5}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:19,504]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:20,840]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:22,508]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:24,470]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:25,776]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:31,440]\u001b[0m Trial 34 finished with value: 0.2357142857142857 and parameters: {'n_epochs': 153, 'batch_size': 6, 'learning_rate': 0.004748949813463504, 'optimizer': 'SGD', 'n_layers': 2, 'n_units_l0': 5, 'n_units_l1': 5}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:31,842]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:32,064]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:32,271]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:34,119]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:34,318]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:38,995]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:39,764]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:40,762]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:42,637]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:43,529]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:45,473]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:46,025]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:55,690]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:56,859]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:43:57,552]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:05,219]\u001b[0m Trial 50 finished with value: 0.7928571428571428 and parameters: {'n_epochs': 117, 'batch_size': 6, 'learning_rate': 0.0019977843181038456, 'optimizer': 'Adam', 'n_layers': 4, 'n_units_l0': 9, 'n_units_l1': 6, 'n_units_l2': 6, 'n_units_l3': 8}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:06,825]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:10,433]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:10,915]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:13,006]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:15,087]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:15,497]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:17,618]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:18,042]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:25,116]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:25,874]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:27,188]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:29,812]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:30,376]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:31,526]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:32,020]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:36,232]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:42,477]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:44,032]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:44,642]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:46,678]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:44:55,249]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:06,037]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:10,174]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:20,600]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:23,132]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:53,932]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:54,814]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:54,950]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:45:57,246]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:04,423]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:04,711]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:05,043]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:13,253]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:14,251]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:18,558]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:20,648]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:20,854]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:28,118]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:31,777]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:35,910]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:46,739]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:46:47,509]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:47:18,575]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:47:22,057]\u001b[0m Trial 94 finished with value: 0.3 and parameters: {'n_epochs': 63, 'batch_size': 6, 'learning_rate': 6.90911486051808e-06, 'optimizer': 'Adam', 'n_layers': 3, 'n_units_l0': 4, 'n_units_l1': 5, 'n_units_l2': 4}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:47:25,319]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:47:27,228]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:48:16,854]\u001b[0m Trial 97 finished with value: 0.7428571428571429 and parameters: {'n_epochs': 72, 'batch_size': 2, 'learning_rate': 5.721434634337329e-06, 'optimizer': 'Adam', 'n_layers': 3, 'n_units_l0': 10, 'n_units_l1': 5, 'n_units_l2': 10}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:48:17,470]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:48:21,086]\u001b[0m Trial 99 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:48:24,852]\u001b[0m Trial 100 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:48:25,935]\u001b[0m Trial 101 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:48:29,104]\u001b[0m Trial 102 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:00,974]\u001b[0m Trial 103 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:01,598]\u001b[0m Trial 104 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:05,747]\u001b[0m Trial 105 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:37,993]\u001b[0m Trial 106 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:41,124]\u001b[0m Trial 107 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:41,723]\u001b[0m Trial 108 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:41,907]\u001b[0m Trial 109 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:45,894]\u001b[0m Trial 110 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:48,473]\u001b[0m Trial 111 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:51,663]\u001b[0m Trial 112 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:56,291]\u001b[0m Trial 113 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:49:57,364]\u001b[0m Trial 114 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:00,489]\u001b[0m Trial 115 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:04,372]\u001b[0m Trial 116 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:05,219]\u001b[0m Trial 117 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:06,376]\u001b[0m Trial 118 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:16,388]\u001b[0m Trial 119 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:28,022]\u001b[0m Trial 120 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:28,497]\u001b[0m Trial 121 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:50:30,468]\u001b[0m Trial 122 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:51:50,679]\u001b[0m Trial 123 finished with value: 0.7214285714285714 and parameters: {'n_epochs': 81, 'batch_size': 1, 'learning_rate': 0.017516230924389458, 'optimizer': 'SGD', 'n_layers': 4, 'n_units_l0': 5, 'n_units_l1': 8, 'n_units_l2': 7, 'n_units_l3': 10}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:51:51,530]\u001b[0m Trial 124 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:52:21,451]\u001b[0m Trial 125 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:52:21,783]\u001b[0m Trial 126 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:52:24,987]\u001b[0m Trial 127 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:52:26,860]\u001b[0m Trial 128 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:52:27,319]\u001b[0m Trial 129 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:52:29,178]\u001b[0m Trial 130 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:17,292]\u001b[0m Trial 131 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:19,639]\u001b[0m Trial 132 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:34,176]\u001b[0m Trial 133 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:39,746]\u001b[0m Trial 134 finished with value: 0.8285714285714286 and parameters: {'n_epochs': 155, 'batch_size': 6, 'learning_rate': 0.03820267505088879, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 7}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:39,922]\u001b[0m Trial 135 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:40,096]\u001b[0m Trial 136 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:41,850]\u001b[0m Trial 137 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:42,353]\u001b[0m Trial 138 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:50,727]\u001b[0m Trial 139 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:54,124]\u001b[0m Trial 140 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:57,561]\u001b[0m Trial 141 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:53:59,280]\u001b[0m Trial 142 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:01,102]\u001b[0m Trial 143 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:13,628]\u001b[0m Trial 144 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:22,117]\u001b[0m Trial 145 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:23,861]\u001b[0m Trial 146 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:27,282]\u001b[0m Trial 147 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:27,750]\u001b[0m Trial 148 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:32,922]\u001b[0m Trial 149 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:34,663]\u001b[0m Trial 150 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:38,847]\u001b[0m Trial 151 finished with value: 0.7428571428571429 and parameters: {'n_epochs': 40, 'batch_size': 5, 'learning_rate': 0.09281869106657309, 'optimizer': 'Adam', 'n_layers': 4, 'n_units_l0': 10, 'n_units_l1': 8, 'n_units_l2': 4, 'n_units_l3': 10}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:39,128]\u001b[0m Trial 152 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:44,394]\u001b[0m Trial 153 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:45,226]\u001b[0m Trial 154 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:46,528]\u001b[0m Trial 155 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:46,739]\u001b[0m Trial 156 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:47,032]\u001b[0m Trial 157 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:48,659]\u001b[0m Trial 158 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:49,398]\u001b[0m Trial 159 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:54:49,948]\u001b[0m Trial 160 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:56:37,899]\u001b[0m Trial 161 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 121, 'batch_size': 2, 'learning_rate': 0.004688349767408032, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 8, 'n_units_l1': 8, 'n_units_l2': 10, 'n_units_l3': 4, 'n_units_l4': 6}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:56:38,067]\u001b[0m Trial 162 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:57:18,564]\u001b[0m Trial 163 finished with value: 0.75 and parameters: {'n_epochs': 39, 'batch_size': 2, 'learning_rate': 0.031086313063022012, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 5, 'n_units_l1': 10, 'n_units_l2': 8, 'n_units_l3': 9, 'n_units_l4': 4, 'n_units_l5': 7}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:57:21,637]\u001b[0m Trial 164 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:57:38,345]\u001b[0m Trial 165 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:57:52,801]\u001b[0m Trial 166 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:58:16,265]\u001b[0m Trial 167 finished with value: 0.8 and parameters: {'n_epochs': 175, 'batch_size': 5, 'learning_rate': 0.0003927380079714117, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 10, 'n_units_l1': 8, 'n_units_l2': 7, 'n_units_l3': 6, 'n_units_l4': 10, 'n_units_l5': 10}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:58:39,043]\u001b[0m Trial 168 finished with value: 0.75 and parameters: {'n_epochs': 75, 'batch_size': 3, 'learning_rate': 2.1784183675175194e-05, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 10, 'n_units_l1': 4, 'n_units_l2': 6, 'n_units_l3': 8}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:58:40,828]\u001b[0m Trial 169 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:58:42,598]\u001b[0m Trial 170 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:58:54,074]\u001b[0m Trial 171 finished with value: 0.7928571428571428 and parameters: {'n_epochs': 179, 'batch_size': 6, 'learning_rate': 0.009965607525471265, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 6, 'n_units_l1': 10, 'n_units_l2': 7, 'n_units_l3': 8, 'n_units_l4': 9}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:00,407]\u001b[0m Trial 172 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:01,307]\u001b[0m Trial 173 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:09,913]\u001b[0m Trial 174 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:11,662]\u001b[0m Trial 175 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:16,963]\u001b[0m Trial 176 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:18,701]\u001b[0m Trial 177 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 15:59:43,390]\u001b[0m Trial 178 finished with value: 0.8071428571428572 and parameters: {'n_epochs': 154, 'batch_size': 4, 'learning_rate': 0.0001389515665019397, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 6, 'n_units_l1': 7, 'n_units_l2': 7, 'n_units_l3': 6}. Best is trial 9 with value: 0.8571428571428571.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:00:54,011]\u001b[0m Trial 179 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:01:02,895]\u001b[0m Trial 180 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:01:06,329]\u001b[0m Trial 181 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:01:09,777]\u001b[0m Trial 182 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:01:10,725]\u001b[0m Trial 183 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:01:11,221]\u001b[0m Trial 184 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:26,547]\u001b[0m Trial 185 finished with value: 0.8785714285714286 and parameters: {'n_epochs': 74, 'batch_size': 1, 'learning_rate': 0.022080794224197335, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 5, 'n_units_l1': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:30,652]\u001b[0m Trial 186 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:43,289]\u001b[0m Trial 187 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:49,473]\u001b[0m Trial 188 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:51,544]\u001b[0m Trial 189 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:53,612]\u001b[0m Trial 190 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:02:57,500]\u001b[0m Trial 191 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:02,250]\u001b[0m Trial 192 finished with value: 0.7928571428571428 and parameters: {'n_epochs': 64, 'batch_size': 5, 'learning_rate': 3.7239362807215797e-06, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:04,359]\u001b[0m Trial 193 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:04,568]\u001b[0m Trial 194 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:07,580]\u001b[0m Trial 195 finished with value: 0.8 and parameters: {'n_epochs': 40, 'batch_size': 5, 'learning_rate': 2.3848265060919034e-05, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:11,686]\u001b[0m Trial 196 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:15,810]\u001b[0m Trial 197 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:03:16,134]\u001b[0m Trial 198 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:31,818]\u001b[0m Trial 199 finished with value: 0.8 and parameters: {'n_epochs': 148, 'batch_size': 2, 'learning_rate': 0.00010240212188612947, 'optimizer': 'RMSprop', 'n_layers': 3, 'n_units_l0': 5, 'n_units_l1': 5, 'n_units_l2': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:32,007]\u001b[0m Trial 200 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:32,403]\u001b[0m Trial 201 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:45,097]\u001b[0m Trial 202 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:45,820]\u001b[0m Trial 203 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:47,908]\u001b[0m Trial 204 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:53,642]\u001b[0m Trial 205 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:04:57,764]\u001b[0m Trial 206 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:05:01,182]\u001b[0m Trial 207 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:05:05,646]\u001b[0m Trial 208 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:05:06,050]\u001b[0m Trial 209 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:05:13,514]\u001b[0m Trial 210 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:05:13,758]\u001b[0m Trial 211 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:08:59,722]\u001b[0m Trial 212 finished with value: 0.7714285714285715 and parameters: {'n_epochs': 131, 'batch_size': 1, 'learning_rate': 0.014534861872567828, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 9, 'n_units_l1': 8, 'n_units_l2': 8, 'n_units_l3': 5, 'n_units_l4': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:01,434]\u001b[0m Trial 213 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:03,505]\u001b[0m Trial 214 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:03,752]\u001b[0m Trial 215 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:04,265]\u001b[0m Trial 216 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:06,001]\u001b[0m Trial 217 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:06,287]\u001b[0m Trial 218 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:18,167]\u001b[0m Trial 219 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:22,231]\u001b[0m Trial 220 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:49,699]\u001b[0m Trial 221 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:09:51,726]\u001b[0m Trial 222 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:10:04,702]\u001b[0m Trial 223 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:10:05,979]\u001b[0m Trial 224 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:10:07,410]\u001b[0m Trial 225 finished with value: 0.5428571428571428 and parameters: {'n_epochs': 35, 'batch_size': 6, 'learning_rate': 2.049718322681704e-06, 'optimizer': 'SGD', 'n_layers': 3, 'n_units_l0': 6, 'n_units_l1': 10, 'n_units_l2': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:10:08,125]\u001b[0m Trial 226 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:10:37,112]\u001b[0m Trial 227 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:10:37,814]\u001b[0m Trial 228 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:11:39,765]\u001b[0m Trial 229 finished with value: 0.8214285714285714 and parameters: {'n_epochs': 62, 'batch_size': 1, 'learning_rate': 0.002562814002220502, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 7, 'n_units_l1': 9}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:12:38,654]\u001b[0m Trial 230 finished with value: 0.8071428571428572 and parameters: {'n_epochs': 113, 'batch_size': 3, 'learning_rate': 0.0014976710580628081, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 8, 'n_units_l1': 10, 'n_units_l2': 10, 'n_units_l3': 5, 'n_units_l4': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:12:49,070]\u001b[0m Trial 231 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:16,652]\u001b[0m Trial 232 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:17,260]\u001b[0m Trial 233 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:20,147]\u001b[0m Trial 234 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:23,054]\u001b[0m Trial 235 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:25,874]\u001b[0m Trial 236 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:28,837]\u001b[0m Trial 237 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:39,996]\u001b[0m Trial 238 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:42,115]\u001b[0m Trial 239 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:44,383]\u001b[0m Trial 240 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:46,383]\u001b[0m Trial 241 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:50,527]\u001b[0m Trial 242 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:52,291]\u001b[0m Trial 243 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:52,437]\u001b[0m Trial 244 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:56,508]\u001b[0m Trial 245 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:13:59,109]\u001b[0m Trial 246 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:02,290]\u001b[0m Trial 247 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:03,238]\u001b[0m Trial 248 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:03,757]\u001b[0m Trial 249 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:06,438]\u001b[0m Trial 250 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:09,957]\u001b[0m Trial 251 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:34,998]\u001b[0m Trial 252 finished with value: 0.8 and parameters: {'n_epochs': 90, 'batch_size': 4, 'learning_rate': 0.02822660366709504, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 9, 'n_units_l1': 6, 'n_units_l2': 8, 'n_units_l3': 7, 'n_units_l4': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:35,554]\u001b[0m Trial 253 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:38,072]\u001b[0m Trial 254 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:14:59,946]\u001b[0m Trial 255 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:02,348]\u001b[0m Trial 256 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:06,434]\u001b[0m Trial 257 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:07,786]\u001b[0m Trial 258 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:11,377]\u001b[0m Trial 259 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:11,728]\u001b[0m Trial 260 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:22,001]\u001b[0m Trial 261 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:32,153]\u001b[0m Trial 262 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:43,753]\u001b[0m Trial 263 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:45,915]\u001b[0m Trial 264 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:15:56,391]\u001b[0m Trial 265 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:16:46,904]\u001b[0m Trial 266 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:16:57,399]\u001b[0m Trial 267 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:16:58,748]\u001b[0m Trial 268 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:01,768]\u001b[0m Trial 269 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:19,320]\u001b[0m Trial 270 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:26,492]\u001b[0m Trial 271 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:28,628]\u001b[0m Trial 272 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:40,535]\u001b[0m Trial 273 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:45,208]\u001b[0m Trial 274 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:46,357]\u001b[0m Trial 275 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:48,933]\u001b[0m Trial 276 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:17:49,802]\u001b[0m Trial 277 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:01,372]\u001b[0m Trial 278 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:02,272]\u001b[0m Trial 279 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:03,996]\u001b[0m Trial 280 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:04,862]\u001b[0m Trial 281 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:06,439]\u001b[0m Trial 282 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:12,425]\u001b[0m Trial 283 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:25,448]\u001b[0m Trial 284 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:26,636]\u001b[0m Trial 285 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:27,570]\u001b[0m Trial 286 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:29,643]\u001b[0m Trial 287 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:30,633]\u001b[0m Trial 288 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:32,009]\u001b[0m Trial 289 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:32,648]\u001b[0m Trial 290 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:35,178]\u001b[0m Trial 291 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:35,400]\u001b[0m Trial 292 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:38,608]\u001b[0m Trial 293 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:48,790]\u001b[0m Trial 294 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:50,104]\u001b[0m Trial 295 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:50,332]\u001b[0m Trial 296 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:18:50,562]\u001b[0m Trial 297 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:19:28,549]\u001b[0m Trial 298 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:19:30,651]\u001b[0m Trial 299 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:19:59,040]\u001b[0m Trial 300 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:19:59,739]\u001b[0m Trial 301 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:10,009]\u001b[0m Trial 302 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:10,515]\u001b[0m Trial 303 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:20,748]\u001b[0m Trial 304 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:22,868]\u001b[0m Trial 305 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:23,083]\u001b[0m Trial 306 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:23,309]\u001b[0m Trial 307 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:33,487]\u001b[0m Trial 308 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:35,667]\u001b[0m Trial 309 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:41,915]\u001b[0m Trial 310 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:20:52,132]\u001b[0m Trial 311 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:21:06,617]\u001b[0m Trial 312 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:21:07,049]\u001b[0m Trial 313 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:21:10,923]\u001b[0m Trial 314 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:21:12,657]\u001b[0m Trial 315 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:21:33,095]\u001b[0m Trial 316 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:21:33,597]\u001b[0m Trial 317 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:22:01,832]\u001b[0m Trial 318 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:22:14,376]\u001b[0m Trial 319 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:27,118]\u001b[0m Trial 320 finished with value: 0.8285714285714286 and parameters: {'n_epochs': 140, 'batch_size': 3, 'learning_rate': 0.011480749108783585, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 9, 'n_units_l1': 8, 'n_units_l2': 8, 'n_units_l3': 4, 'n_units_l4': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:27,486]\u001b[0m Trial 321 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:29,678]\u001b[0m Trial 322 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:31,375]\u001b[0m Trial 323 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:32,061]\u001b[0m Trial 324 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:32,496]\u001b[0m Trial 325 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:23:35,105]\u001b[0m Trial 326 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:24:50,335]\u001b[0m Trial 327 finished with value: 0.8214285714285714 and parameters: {'n_epochs': 142, 'batch_size': 3, 'learning_rate': 0.005218345104778139, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 9, 'n_units_l1': 7, 'n_units_l2': 8, 'n_units_l3': 4, 'n_units_l4': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:24:52,505]\u001b[0m Trial 328 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:03,593]\u001b[0m Trial 329 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:05,741]\u001b[0m Trial 330 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:18,508]\u001b[0m Trial 331 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:20,311]\u001b[0m Trial 332 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:25,318]\u001b[0m Trial 333 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:27,828]\u001b[0m Trial 334 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:32,756]\u001b[0m Trial 335 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:34,030]\u001b[0m Trial 336 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:44,447]\u001b[0m Trial 337 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:44,820]\u001b[0m Trial 338 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:46,987]\u001b[0m Trial 339 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:51,151]\u001b[0m Trial 340 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:53,731]\u001b[0m Trial 341 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:55,884]\u001b[0m Trial 342 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:56,722]\u001b[0m Trial 343 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:25:57,921]\u001b[0m Trial 344 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:27:40,488]\u001b[0m Trial 345 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 101, 'batch_size': 1, 'learning_rate': 0.004560999475336571, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 8, 'n_units_l1': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:27:44,703]\u001b[0m Trial 346 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:27:59,491]\u001b[0m Trial 347 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:27:59,713]\u001b[0m Trial 348 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:00,350]\u001b[0m Trial 349 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:01,898]\u001b[0m Trial 350 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:02,325]\u001b[0m Trial 351 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:08,480]\u001b[0m Trial 352 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:12,643]\u001b[0m Trial 353 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:13,289]\u001b[0m Trial 354 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:24,003]\u001b[0m Trial 355 finished with value: 0.7714285714285715 and parameters: {'n_epochs': 117, 'batch_size': 6, 'learning_rate': 0.019491067525448026, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 7, 'n_units_l1': 5, 'n_units_l2': 8, 'n_units_l3': 6, 'n_units_l4': 9, 'n_units_l5': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:24,448]\u001b[0m Trial 356 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:24,810]\u001b[0m Trial 357 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:27,315]\u001b[0m Trial 358 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:27,692]\u001b[0m Trial 359 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:28,080]\u001b[0m Trial 360 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:28,457]\u001b[0m Trial 361 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:28,892]\u001b[0m Trial 362 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:31,419]\u001b[0m Trial 363 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:42,266]\u001b[0m Trial 364 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:47,957]\u001b[0m Trial 365 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:55,158]\u001b[0m Trial 366 finished with value: 0.8 and parameters: {'n_epochs': 54, 'batch_size': 5, 'learning_rate': 0.0010806253393203335, 'optimizer': 'Adam', 'n_layers': 4, 'n_units_l0': 10, 'n_units_l1': 10, 'n_units_l2': 9, 'n_units_l3': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:55,894]\u001b[0m Trial 367 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:28:58,563]\u001b[0m Trial 368 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:16,423]\u001b[0m Trial 369 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:28,564]\u001b[0m Trial 370 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:35,194]\u001b[0m Trial 371 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:39,352]\u001b[0m Trial 372 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:42,940]\u001b[0m Trial 373 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:44,089]\u001b[0m Trial 374 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:50,302]\u001b[0m Trial 375 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:51,627]\u001b[0m Trial 376 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:52,440]\u001b[0m Trial 377 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:55,038]\u001b[0m Trial 378 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:55,368]\u001b[0m Trial 379 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:56,021]\u001b[0m Trial 380 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:57,824]\u001b[0m Trial 381 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:29:58,045]\u001b[0m Trial 382 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:16,268]\u001b[0m Trial 383 finished with value: 0.8428571428571429 and parameters: {'n_epochs': 110, 'batch_size': 4, 'learning_rate': 0.021274171486218955, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 7, 'n_units_l1': 4}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:17,175]\u001b[0m Trial 384 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:21,365]\u001b[0m Trial 385 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:35,158]\u001b[0m Trial 386 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:37,315]\u001b[0m Trial 387 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:39,477]\u001b[0m Trial 388 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:41,669]\u001b[0m Trial 389 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:43,854]\u001b[0m Trial 390 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:45,557]\u001b[0m Trial 391 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:54,286]\u001b[0m Trial 392 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:30:56,466]\u001b[0m Trial 393 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:31:09,079]\u001b[0m Trial 394 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:31:11,624]\u001b[0m Trial 395 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:31:21,914]\u001b[0m Trial 396 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:31:32,537]\u001b[0m Trial 397 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:17,908]\u001b[0m Trial 398 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:21,295]\u001b[0m Trial 399 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:24,682]\u001b[0m Trial 400 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:42,269]\u001b[0m Trial 401 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:46,997]\u001b[0m Trial 402 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:49,083]\u001b[0m Trial 403 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:32:50,094]\u001b[0m Trial 404 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:03,016]\u001b[0m Trial 405 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:06,084]\u001b[0m Trial 406 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:08,628]\u001b[0m Trial 407 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:11,207]\u001b[0m Trial 408 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:13,140]\u001b[0m Trial 409 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:17,260]\u001b[0m Trial 410 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:19,691]\u001b[0m Trial 411 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:21,970]\u001b[0m Trial 412 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:25,423]\u001b[0m Trial 413 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:26,951]\u001b[0m Trial 414 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:39,493]\u001b[0m Trial 415 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:42,725]\u001b[0m Trial 416 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:45,267]\u001b[0m Trial 417 finished with value: 0.7285714285714285 and parameters: {'n_epochs': 55, 'batch_size': 6, 'learning_rate': 0.0015117028720395588, 'optimizer': 'SGD', 'n_layers': 4, 'n_units_l0': 9, 'n_units_l1': 7, 'n_units_l2': 5, 'n_units_l3': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:47,400]\u001b[0m Trial 418 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:48,183]\u001b[0m Trial 419 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:50,325]\u001b[0m Trial 420 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:53,070]\u001b[0m Trial 421 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:53,398]\u001b[0m Trial 422 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:54,315]\u001b[0m Trial 423 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:58,837]\u001b[0m Trial 424 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:33:59,278]\u001b[0m Trial 425 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:34:12,157]\u001b[0m Trial 426 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:34:24,451]\u001b[0m Trial 427 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:34:33,337]\u001b[0m Trial 428 finished with value: 0.8071428571428572 and parameters: {'n_epochs': 77, 'batch_size': 5, 'learning_rate': 0.005964118290183055, 'optimizer': 'RMSprop', 'n_layers': 5, 'n_units_l0': 10, 'n_units_l1': 10, 'n_units_l2': 10, 'n_units_l3': 5, 'n_units_l4': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:34:38,480]\u001b[0m Trial 429 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:17,120]\u001b[0m Trial 430 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 178, 'batch_size': 4, 'learning_rate': 2.2686576819101577e-05, 'optimizer': 'RMSprop', 'n_layers': 5, 'n_units_l0': 10, 'n_units_l1': 9, 'n_units_l2': 5, 'n_units_l3': 8, 'n_units_l4': 9}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:21,475]\u001b[0m Trial 431 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:21,734]\u001b[0m Trial 432 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:24,407]\u001b[0m Trial 433 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:39,250]\u001b[0m Trial 434 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:39,843]\u001b[0m Trial 435 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:42,012]\u001b[0m Trial 436 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:52,221]\u001b[0m Trial 437 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:54,909]\u001b[0m Trial 438 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:55,319]\u001b[0m Trial 439 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:35:57,818]\u001b[0m Trial 440 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:15,180]\u001b[0m Trial 441 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:15,938]\u001b[0m Trial 442 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:26,162]\u001b[0m Trial 443 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:28,250]\u001b[0m Trial 444 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:28,676]\u001b[0m Trial 445 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:31,190]\u001b[0m Trial 446 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:34,344]\u001b[0m Trial 447 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:39,207]\u001b[0m Trial 448 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:41,336]\u001b[0m Trial 449 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:44,391]\u001b[0m Trial 450 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:46,499]\u001b[0m Trial 451 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:48,580]\u001b[0m Trial 452 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:36:54,726]\u001b[0m Trial 453 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:22,619]\u001b[0m Trial 454 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:25,869]\u001b[0m Trial 455 finished with value: 0.8214285714285714 and parameters: {'n_epochs': 65, 'batch_size': 6, 'learning_rate': 0.03206519005852203, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 5, 'n_units_l1': 4}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:28,328]\u001b[0m Trial 456 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:28,616]\u001b[0m Trial 457 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:30,867]\u001b[0m Trial 458 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:41,425]\u001b[0m Trial 459 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:43,604]\u001b[0m Trial 460 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:56,677]\u001b[0m Trial 461 finished with value: 0.8214285714285714 and parameters: {'n_epochs': 173, 'batch_size': 6, 'learning_rate': 0.00819372311268445, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 5, 'n_units_l1': 6, 'n_units_l2': 7, 'n_units_l3': 7, 'n_units_l4': 9}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:57,282]\u001b[0m Trial 462 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:57,929]\u001b[0m Trial 463 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:37:58,527]\u001b[0m Trial 464 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:01,087]\u001b[0m Trial 465 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:03,223]\u001b[0m Trial 466 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:16,504]\u001b[0m Trial 467 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:16,822]\u001b[0m Trial 468 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:19,757]\u001b[0m Trial 469 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:24,103]\u001b[0m Trial 470 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:24,820]\u001b[0m Trial 471 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:53,156]\u001b[0m Trial 472 finished with value: 0.8 and parameters: {'n_epochs': 170, 'batch_size': 5, 'learning_rate': 0.0034026440182587624, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 5, 'n_units_l1': 6, 'n_units_l2': 6, 'n_units_l3': 7, 'n_units_l4': 7, 'n_units_l5': 4}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:57,386]\u001b[0m Trial 473 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:38:57,625]\u001b[0m Trial 474 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:03,688]\u001b[0m Trial 475 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:06,794]\u001b[0m Trial 476 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:08,238]\u001b[0m Trial 477 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:10,078]\u001b[0m Trial 478 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:12,849]\u001b[0m Trial 479 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:15,080]\u001b[0m Trial 480 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:15,415]\u001b[0m Trial 481 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:17,740]\u001b[0m Trial 482 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:19,777]\u001b[0m Trial 483 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:21,803]\u001b[0m Trial 484 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:22,181]\u001b[0m Trial 485 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:39:26,527]\u001b[0m Trial 486 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:40:09,690]\u001b[0m Trial 487 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:40:11,471]\u001b[0m Trial 488 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:40:30,206]\u001b[0m Trial 489 finished with value: 0.7571428571428571 and parameters: {'n_epochs': 92, 'batch_size': 4, 'learning_rate': 0.022768596645066738, 'optimizer': 'RMSprop', 'n_layers': 6, 'n_units_l0': 5, 'n_units_l1': 5, 'n_units_l2': 8, 'n_units_l3': 5, 'n_units_l4': 7, 'n_units_l5': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:40:33,666]\u001b[0m Trial 490 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:40:35,203]\u001b[0m Trial 491 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:40:58,950]\u001b[0m Trial 492 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:01,160]\u001b[0m Trial 493 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:04,359]\u001b[0m Trial 494 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:09,229]\u001b[0m Trial 495 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:10,537]\u001b[0m Trial 496 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:12,652]\u001b[0m Trial 497 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:23,590]\u001b[0m Trial 498 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:23,842]\u001b[0m Trial 499 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:29,913]\u001b[0m Trial 500 finished with value: 0.8214285714285714 and parameters: {'n_epochs': 51, 'batch_size': 5, 'learning_rate': 0.002337446976655579, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 10, 'n_units_l1': 10, 'n_units_l2': 9, 'n_units_l3': 7, 'n_units_l4': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:41:30,709]\u001b[0m Trial 501 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:42:39,660]\u001b[0m Trial 502 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:42:41,048]\u001b[0m Trial 503 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:42:42,809]\u001b[0m Trial 504 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:42:43,106]\u001b[0m Trial 505 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:06,911]\u001b[0m Trial 506 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:35,963]\u001b[0m Trial 507 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:36,564]\u001b[0m Trial 508 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:37,172]\u001b[0m Trial 509 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:49,124]\u001b[0m Trial 510 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:50,243]\u001b[0m Trial 511 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:51,553]\u001b[0m Trial 512 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:53,312]\u001b[0m Trial 513 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:43:53,512]\u001b[0m Trial 514 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:44:22,292]\u001b[0m Trial 515 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:44:26,434]\u001b[0m Trial 516 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:44:30,729]\u001b[0m Trial 517 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:11,651]\u001b[0m Trial 518 finished with value: 0.7642857142857142 and parameters: {'n_epochs': 174, 'batch_size': 4, 'learning_rate': 1.1302533902537418e-06, 'optimizer': 'RMSprop', 'n_layers': 6, 'n_units_l0': 9, 'n_units_l1': 9, 'n_units_l2': 5, 'n_units_l3': 9, 'n_units_l4': 9, 'n_units_l5': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:14,301]\u001b[0m Trial 519 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:20,389]\u001b[0m Trial 520 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:23,377]\u001b[0m Trial 521 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:27,996]\u001b[0m Trial 522 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:28,236]\u001b[0m Trial 523 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:28,684]\u001b[0m Trial 524 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:30,680]\u001b[0m Trial 525 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:34,881]\u001b[0m Trial 526 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:35,404]\u001b[0m Trial 527 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:35,615]\u001b[0m Trial 528 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:39,766]\u001b[0m Trial 529 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:40,077]\u001b[0m Trial 530 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:45:49,152]\u001b[0m Trial 531 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:46:17,206]\u001b[0m Trial 532 finished with value: 0.8428571428571429 and parameters: {'n_epochs': 170, 'batch_size': 5, 'learning_rate': 0.002467068903479824, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 4, 'n_units_l1': 7, 'n_units_l2': 5, 'n_units_l3': 7, 'n_units_l4': 8, 'n_units_l5': 4}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:46:19,448]\u001b[0m Trial 533 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:46:23,868]\u001b[0m Trial 534 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:46:25,841]\u001b[0m Trial 535 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:46:26,213]\u001b[0m Trial 536 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:46:59,309]\u001b[0m Trial 537 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:01,768]\u001b[0m Trial 538 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:01,989]\u001b[0m Trial 539 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:05,297]\u001b[0m Trial 540 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:08,559]\u001b[0m Trial 541 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:08,916]\u001b[0m Trial 542 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:09,723]\u001b[0m Trial 543 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:12,180]\u001b[0m Trial 544 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:12,713]\u001b[0m Trial 545 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:24,659]\u001b[0m Trial 546 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:25,327]\u001b[0m Trial 547 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:31,468]\u001b[0m Trial 548 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:32,472]\u001b[0m Trial 549 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:35,070]\u001b[0m Trial 550 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:36,424]\u001b[0m Trial 551 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:51,707]\u001b[0m Trial 552 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:55,653]\u001b[0m Trial 553 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:57,860]\u001b[0m Trial 554 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:47:58,979]\u001b[0m Trial 555 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:12,265]\u001b[0m Trial 556 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:15,989]\u001b[0m Trial 557 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:16,275]\u001b[0m Trial 558 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:16,611]\u001b[0m Trial 559 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:20,797]\u001b[0m Trial 560 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:31,432]\u001b[0m Trial 561 finished with value: 0.8214285714285714 and parameters: {'n_epochs': 81, 'batch_size': 4, 'learning_rate': 0.0026360461689202527, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 10, 'n_units_l1': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:32,383]\u001b[0m Trial 562 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:33,038]\u001b[0m Trial 563 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:33,667]\u001b[0m Trial 564 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:34,001]\u001b[0m Trial 565 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:37,502]\u001b[0m Trial 566 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:38,891]\u001b[0m Trial 567 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:39,591]\u001b[0m Trial 568 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:40,002]\u001b[0m Trial 569 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:40,812]\u001b[0m Trial 570 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:41,124]\u001b[0m Trial 571 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:41,726]\u001b[0m Trial 572 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:45,082]\u001b[0m Trial 573 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:46,081]\u001b[0m Trial 574 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:47,392]\u001b[0m Trial 575 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:54,224]\u001b[0m Trial 576 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 117, 'batch_size': 6, 'learning_rate': 0.0009110741454386279, 'optimizer': 'RMSprop', 'n_layers': 5, 'n_units_l0': 8, 'n_units_l1': 8, 'n_units_l2': 4, 'n_units_l3': 7, 'n_units_l4': 10}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:54,559]\u001b[0m Trial 577 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:55,069]\u001b[0m Trial 578 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:48:57,580]\u001b[0m Trial 579 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:20,237]\u001b[0m Trial 580 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:20,565]\u001b[0m Trial 581 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:20,974]\u001b[0m Trial 582 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:26,097]\u001b[0m Trial 583 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:26,660]\u001b[0m Trial 584 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:31,186]\u001b[0m Trial 585 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:31,507]\u001b[0m Trial 586 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:31,991]\u001b[0m Trial 587 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:33,863]\u001b[0m Trial 588 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:35,147]\u001b[0m Trial 589 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:36,487]\u001b[0m Trial 590 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:38,557]\u001b[0m Trial 591 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:40,349]\u001b[0m Trial 592 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:40,763]\u001b[0m Trial 593 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:41,301]\u001b[0m Trial 594 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:49:50,831]\u001b[0m Trial 595 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:04,080]\u001b[0m Trial 596 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:11,732]\u001b[0m Trial 597 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:18,259]\u001b[0m Trial 598 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:18,419]\u001b[0m Trial 599 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:24,082]\u001b[0m Trial 600 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:24,338]\u001b[0m Trial 601 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:26,318]\u001b[0m Trial 602 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:27,792]\u001b[0m Trial 603 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:32,060]\u001b[0m Trial 604 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:34,250]\u001b[0m Trial 605 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:34,681]\u001b[0m Trial 606 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:37,347]\u001b[0m Trial 607 finished with value: 0.7714285714285715 and parameters: {'n_epochs': 37, 'batch_size': 6, 'learning_rate': 0.0008640718308380943, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 5, 'n_units_l1': 7, 'n_units_l2': 7, 'n_units_l3': 6, 'n_units_l4': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:45,573]\u001b[0m Trial 608 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:47,739]\u001b[0m Trial 609 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:53,596]\u001b[0m Trial 610 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:50:57,904]\u001b[0m Trial 611 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:01,077]\u001b[0m Trial 612 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:13,596]\u001b[0m Trial 613 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:18,210]\u001b[0m Trial 614 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:26,167]\u001b[0m Trial 615 finished with value: 0.7785714285714286 and parameters: {'n_epochs': 54, 'batch_size': 5, 'learning_rate': 0.030172498658994334, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 6, 'n_units_l1': 7, 'n_units_l2': 5, 'n_units_l3': 7, 'n_units_l4': 7, 'n_units_l5': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:27,504]\u001b[0m Trial 616 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:31,736]\u001b[0m Trial 617 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:35,710]\u001b[0m Trial 618 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:39,830]\u001b[0m Trial 619 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:43,436]\u001b[0m Trial 620 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:46,003]\u001b[0m Trial 621 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:50,421]\u001b[0m Trial 622 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:51,293]\u001b[0m Trial 623 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:51,621]\u001b[0m Trial 624 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:52,263]\u001b[0m Trial 625 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:52,685]\u001b[0m Trial 626 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:53,265]\u001b[0m Trial 627 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:53,544]\u001b[0m Trial 628 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:53,854]\u001b[0m Trial 629 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:54,425]\u001b[0m Trial 630 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:54,984]\u001b[0m Trial 631 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:55,457]\u001b[0m Trial 632 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:57,151]\u001b[0m Trial 633 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:57,495]\u001b[0m Trial 634 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:51:57,812]\u001b[0m Trial 635 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:02,577]\u001b[0m Trial 636 finished with value: 0.8 and parameters: {'n_epochs': 36, 'batch_size': 4, 'learning_rate': 0.00047053210127079314, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 4}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:04,719]\u001b[0m Trial 637 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:05,304]\u001b[0m Trial 638 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:14,590]\u001b[0m Trial 639 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:17,840]\u001b[0m Trial 640 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:27,025]\u001b[0m Trial 641 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:28,031]\u001b[0m Trial 642 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:29,990]\u001b[0m Trial 643 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:32,477]\u001b[0m Trial 644 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:32,783]\u001b[0m Trial 645 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:52:36,948]\u001b[0m Trial 646 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:10,136]\u001b[0m Trial 647 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:13,509]\u001b[0m Trial 648 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:45,368]\u001b[0m Trial 649 finished with value: 0.8 and parameters: {'n_epochs': 71, 'batch_size': 3, 'learning_rate': 0.031219882608170123, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 8, 'n_units_l1': 10, 'n_units_l2': 7, 'n_units_l3': 5, 'n_units_l4': 9}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:46,507]\u001b[0m Trial 650 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:48,693]\u001b[0m Trial 651 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:49,943]\u001b[0m Trial 652 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:52,860]\u001b[0m Trial 653 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:53:57,679]\u001b[0m Trial 654 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:00,323]\u001b[0m Trial 655 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:03,108]\u001b[0m Trial 656 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:05,442]\u001b[0m Trial 657 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:09,613]\u001b[0m Trial 658 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:14,413]\u001b[0m Trial 659 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:46,604]\u001b[0m Trial 660 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:57,588]\u001b[0m Trial 661 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:54:57,901]\u001b[0m Trial 662 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:00,116]\u001b[0m Trial 663 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:11,436]\u001b[0m Trial 664 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:12,078]\u001b[0m Trial 665 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:16,688]\u001b[0m Trial 666 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:19,113]\u001b[0m Trial 667 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:25,250]\u001b[0m Trial 668 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:27,536]\u001b[0m Trial 669 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:30,112]\u001b[0m Trial 670 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:32,461]\u001b[0m Trial 671 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:34,296]\u001b[0m Trial 672 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:34,597]\u001b[0m Trial 673 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:35,560]\u001b[0m Trial 674 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:35,775]\u001b[0m Trial 675 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:36,343]\u001b[0m Trial 676 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:40,869]\u001b[0m Trial 677 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:56,152]\u001b[0m Trial 678 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:55:59,903]\u001b[0m Trial 679 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:02,695]\u001b[0m Trial 680 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:03,435]\u001b[0m Trial 681 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:07,035]\u001b[0m Trial 682 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:13,174]\u001b[0m Trial 683 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:13,725]\u001b[0m Trial 684 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:20,450]\u001b[0m Trial 685 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:21,083]\u001b[0m Trial 686 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:33,681]\u001b[0m Trial 687 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:39,984]\u001b[0m Trial 688 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:41,360]\u001b[0m Trial 689 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:52,061]\u001b[0m Trial 690 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:53,054]\u001b[0m Trial 691 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:56:53,645]\u001b[0m Trial 692 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:21,066]\u001b[0m Trial 693 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:24,659]\u001b[0m Trial 694 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:24,885]\u001b[0m Trial 695 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:25,259]\u001b[0m Trial 696 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:28,116]\u001b[0m Trial 697 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:29,958]\u001b[0m Trial 698 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:33,554]\u001b[0m Trial 699 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:57:48,252]\u001b[0m Trial 700 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:58:37,803]\u001b[0m Trial 701 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:58:41,152]\u001b[0m Trial 702 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:58:57,164]\u001b[0m Trial 703 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:58:59,770]\u001b[0m Trial 704 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:01,461]\u001b[0m Trial 705 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:03,921]\u001b[0m Trial 706 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:06,822]\u001b[0m Trial 707 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:07,125]\u001b[0m Trial 708 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:08,425]\u001b[0m Trial 709 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:22,260]\u001b[0m Trial 710 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:24,207]\u001b[0m Trial 711 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:26,163]\u001b[0m Trial 712 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:27,087]\u001b[0m Trial 713 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:31,135]\u001b[0m Trial 714 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:36,893]\u001b[0m Trial 715 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:51,181]\u001b[0m Trial 716 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 16:59:52,176]\u001b[0m Trial 717 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:00,153]\u001b[0m Trial 718 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:02,942]\u001b[0m Trial 719 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:14,232]\u001b[0m Trial 720 finished with value: 0.8142857142857143 and parameters: {'n_epochs': 66, 'batch_size': 5, 'learning_rate': 0.019749421538302135, 'optimizer': 'Adam', 'n_layers': 4, 'n_units_l0': 10, 'n_units_l1': 9, 'n_units_l2': 7, 'n_units_l3': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:18,701]\u001b[0m Trial 721 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:21,353]\u001b[0m Trial 722 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:28,215]\u001b[0m Trial 723 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:29,113]\u001b[0m Trial 724 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:34,217]\u001b[0m Trial 725 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:36,978]\u001b[0m Trial 726 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:38,295]\u001b[0m Trial 727 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:41,448]\u001b[0m Trial 728 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:44,103]\u001b[0m Trial 729 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:46,373]\u001b[0m Trial 730 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:00:46,902]\u001b[0m Trial 731 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:01:01,873]\u001b[0m Trial 732 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:12,953]\u001b[0m Trial 733 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:15,956]\u001b[0m Trial 734 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:31,548]\u001b[0m Trial 735 finished with value: 0.8 and parameters: {'n_epochs': 63, 'batch_size': 4, 'learning_rate': 0.00029929202363450076, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 7, 'n_units_l1': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:39,318]\u001b[0m Trial 736 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:40,744]\u001b[0m Trial 737 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:41,593]\u001b[0m Trial 738 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:46,043]\u001b[0m Trial 739 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:54,408]\u001b[0m Trial 740 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:56,731]\u001b[0m Trial 741 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:02:59,334]\u001b[0m Trial 742 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:03:15,105]\u001b[0m Trial 743 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:03:21,632]\u001b[0m Trial 744 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:03:28,837]\u001b[0m Trial 745 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:03:29,863]\u001b[0m Trial 746 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:03:45,957]\u001b[0m Trial 747 finished with value: 0.8571428571428571 and parameters: {'n_epochs': 74, 'batch_size': 4, 'learning_rate': 0.005478862678012138, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 10, 'n_units_l1': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:03:46,654]\u001b[0m Trial 748 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:07,907]\u001b[0m Trial 749 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:08,863]\u001b[0m Trial 750 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:09,359]\u001b[0m Trial 751 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:31,993]\u001b[0m Trial 752 finished with value: 0.8071428571428572 and parameters: {'n_epochs': 103, 'batch_size': 5, 'learning_rate': 0.007018021306899563, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 6, 'n_units_l1': 5, 'n_units_l2': 6, 'n_units_l3': 7, 'n_units_l4': 9, 'n_units_l5': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:52,268]\u001b[0m Trial 753 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:58,732]\u001b[0m Trial 754 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:04:59,349]\u001b[0m Trial 755 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:00,025]\u001b[0m Trial 756 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:00,898]\u001b[0m Trial 757 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:22,736]\u001b[0m Trial 758 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:23,962]\u001b[0m Trial 759 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:31,648]\u001b[0m Trial 760 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:32,952]\u001b[0m Trial 761 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:33,300]\u001b[0m Trial 762 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:42,562]\u001b[0m Trial 763 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:43,600]\u001b[0m Trial 764 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:05:44,408]\u001b[0m Trial 765 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:22,456]\u001b[0m Trial 766 finished with value: 0.7928571428571428 and parameters: {'n_epochs': 149, 'batch_size': 4, 'learning_rate': 4.8355395080967575e-05, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 10, 'n_units_l1': 9, 'n_units_l2': 5, 'n_units_l3': 10}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:23,675]\u001b[0m Trial 767 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:41,279]\u001b[0m Trial 768 finished with value: 0.85 and parameters: {'n_epochs': 65, 'batch_size': 3, 'learning_rate': 0.028724409188231107, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 6, 'n_units_l1': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:49,607]\u001b[0m Trial 769 finished with value: 0.7928571428571428 and parameters: {'n_epochs': 97, 'batch_size': 6, 'learning_rate': 0.008693357654384157, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 8, 'n_units_l1': 8, 'n_units_l2': 6, 'n_units_l3': 6, 'n_units_l4': 9, 'n_units_l5': 4}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:50,514]\u001b[0m Trial 770 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:51,175]\u001b[0m Trial 771 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:53,987]\u001b[0m Trial 772 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:54,453]\u001b[0m Trial 773 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:56,554]\u001b[0m Trial 774 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:57,133]\u001b[0m Trial 775 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:58,047]\u001b[0m Trial 776 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:06:58,578]\u001b[0m Trial 777 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:01,371]\u001b[0m Trial 778 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:02,580]\u001b[0m Trial 779 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:02,825]\u001b[0m Trial 780 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:04,975]\u001b[0m Trial 781 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:05,626]\u001b[0m Trial 782 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:06,006]\u001b[0m Trial 783 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:06,459]\u001b[0m Trial 784 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:10,523]\u001b[0m Trial 785 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:11,321]\u001b[0m Trial 786 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:12,141]\u001b[0m Trial 787 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:12,818]\u001b[0m Trial 788 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:13,382]\u001b[0m Trial 789 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:14,892]\u001b[0m Trial 790 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:16,674]\u001b[0m Trial 791 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:17,193]\u001b[0m Trial 792 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:18,691]\u001b[0m Trial 793 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:19,138]\u001b[0m Trial 794 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:19,948]\u001b[0m Trial 795 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:20,636]\u001b[0m Trial 796 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:23,568]\u001b[0m Trial 797 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:24,215]\u001b[0m Trial 798 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:37,497]\u001b[0m Trial 799 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:51,570]\u001b[0m Trial 800 finished with value: 0.7785714285714286 and parameters: {'n_epochs': 155, 'batch_size': 5, 'learning_rate': 0.0038668418449508252, 'optimizer': 'Adam', 'n_layers': 2, 'n_units_l0': 9, 'n_units_l1': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:51,973]\u001b[0m Trial 801 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:52,344]\u001b[0m Trial 802 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:56,414]\u001b[0m Trial 803 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:56,685]\u001b[0m Trial 804 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:57,700]\u001b[0m Trial 805 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:59,229]\u001b[0m Trial 806 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:07:59,627]\u001b[0m Trial 807 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:00,894]\u001b[0m Trial 808 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:01,281]\u001b[0m Trial 809 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:02,056]\u001b[0m Trial 810 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:02,731]\u001b[0m Trial 811 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:03,270]\u001b[0m Trial 812 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:03,676]\u001b[0m Trial 813 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:04,837]\u001b[0m Trial 814 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:07,665]\u001b[0m Trial 815 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:08,996]\u001b[0m Trial 816 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:10,439]\u001b[0m Trial 817 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:11,790]\u001b[0m Trial 818 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:12,461]\u001b[0m Trial 819 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:13,456]\u001b[0m Trial 820 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:18,248]\u001b[0m Trial 821 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:18,604]\u001b[0m Trial 822 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:19,000]\u001b[0m Trial 823 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:20,400]\u001b[0m Trial 824 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:21,097]\u001b[0m Trial 825 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:21,368]\u001b[0m Trial 826 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:21,767]\u001b[0m Trial 827 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:22,165]\u001b[0m Trial 828 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:23,711]\u001b[0m Trial 829 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:24,649]\u001b[0m Trial 830 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:25,296]\u001b[0m Trial 831 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:26,174]\u001b[0m Trial 832 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:26,435]\u001b[0m Trial 833 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:26,832]\u001b[0m Trial 834 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:31,515]\u001b[0m Trial 835 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:32,908]\u001b[0m Trial 836 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:33,238]\u001b[0m Trial 837 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:37,092]\u001b[0m Trial 838 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:41,326]\u001b[0m Trial 839 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:51,135]\u001b[0m Trial 840 finished with value: 0.7714285714285715 and parameters: {'n_epochs': 60, 'batch_size': 5, 'learning_rate': 0.0036298389510590963, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 6, 'n_units_l1': 7, 'n_units_l2': 4, 'n_units_l3': 7, 'n_units_l4': 8, 'n_units_l5': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:53,771]\u001b[0m Trial 841 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:08:58,890]\u001b[0m Trial 842 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:10,159]\u001b[0m Trial 843 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:10,812]\u001b[0m Trial 844 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:13,006]\u001b[0m Trial 845 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:13,447]\u001b[0m Trial 846 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:15,694]\u001b[0m Trial 847 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:16,107]\u001b[0m Trial 848 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:09:57,847]\u001b[0m Trial 849 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:00,146]\u001b[0m Trial 850 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:07,667]\u001b[0m Trial 851 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:12,759]\u001b[0m Trial 852 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:15,928]\u001b[0m Trial 853 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:19,082]\u001b[0m Trial 854 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:25,248]\u001b[0m Trial 855 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:30,599]\u001b[0m Trial 856 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:46,986]\u001b[0m Trial 857 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:50,117]\u001b[0m Trial 858 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:56,704]\u001b[0m Trial 859 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:10:57,347]\u001b[0m Trial 860 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:12,706]\u001b[0m Trial 861 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:13,101]\u001b[0m Trial 862 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:13,596]\u001b[0m Trial 863 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:18,021]\u001b[0m Trial 864 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:19,588]\u001b[0m Trial 865 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:19,997]\u001b[0m Trial 866 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:20,892]\u001b[0m Trial 867 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:27,416]\u001b[0m Trial 868 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:40,315]\u001b[0m Trial 869 finished with value: 0.8142857142857143 and parameters: {'n_epochs': 70, 'batch_size': 4, 'learning_rate': 0.016509724655926383, 'optimizer': 'RMSprop', 'n_layers': 3, 'n_units_l0': 10, 'n_units_l1': 6, 'n_units_l2': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:40,775]\u001b[0m Trial 870 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:41,747]\u001b[0m Trial 871 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:43,068]\u001b[0m Trial 872 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:44,990]\u001b[0m Trial 873 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:11:58,054]\u001b[0m Trial 874 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:14,796]\u001b[0m Trial 875 finished with value: 0.8 and parameters: {'n_epochs': 95, 'batch_size': 4, 'learning_rate': 0.022614158529612075, 'optimizer': 'RMSprop', 'n_layers': 3, 'n_units_l0': 9, 'n_units_l1': 5, 'n_units_l2': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:15,571]\u001b[0m Trial 876 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:20,279]\u001b[0m Trial 877 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:30,020]\u001b[0m Trial 878 finished with value: 0.7785714285714286 and parameters: {'n_epochs': 57, 'batch_size': 4, 'learning_rate': 0.01313032091676835, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 4, 'n_units_l1': 8, 'n_units_l2': 6, 'n_units_l3': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:30,689]\u001b[0m Trial 879 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:31,070]\u001b[0m Trial 880 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:32,830]\u001b[0m Trial 881 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:33,302]\u001b[0m Trial 882 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:33,693]\u001b[0m Trial 883 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:35,906]\u001b[0m Trial 884 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:46,727]\u001b[0m Trial 885 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:57,906]\u001b[0m Trial 886 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:58,356]\u001b[0m Trial 887 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:12:58,753]\u001b[0m Trial 888 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:13:00,502]\u001b[0m Trial 889 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:13:15,067]\u001b[0m Trial 890 finished with value: 0.8357142857142857 and parameters: {'n_epochs': 58, 'batch_size': 4, 'learning_rate': 0.0032808585246078227, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 7, 'n_units_l1': 8, 'n_units_l2': 6, 'n_units_l3': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:13:18,616]\u001b[0m Trial 891 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:13:46,142]\u001b[0m Trial 892 finished with value: 0.8428571428571429 and parameters: {'n_epochs': 175, 'batch_size': 5, 'learning_rate': 0.01072508042008783, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 5, 'n_units_l1': 4, 'n_units_l2': 7, 'n_units_l3': 8, 'n_units_l4': 8}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:13:46,586]\u001b[0m Trial 893 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:01,206]\u001b[0m Trial 894 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:01,993]\u001b[0m Trial 895 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:04,275]\u001b[0m Trial 896 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:09,185]\u001b[0m Trial 897 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:09,966]\u001b[0m Trial 898 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:10,410]\u001b[0m Trial 899 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:10,857]\u001b[0m Trial 900 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:14,919]\u001b[0m Trial 901 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:19,863]\u001b[0m Trial 902 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:30,788]\u001b[0m Trial 903 finished with value: 0.8071428571428572 and parameters: {'n_epochs': 58, 'batch_size': 4, 'learning_rate': 0.008352585248491149, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 7, 'n_units_l1': 8, 'n_units_l2': 6, 'n_units_l3': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:31,872]\u001b[0m Trial 904 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:33,852]\u001b[0m Trial 905 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:34,290]\u001b[0m Trial 906 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:39,939]\u001b[0m Trial 907 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:40,401]\u001b[0m Trial 908 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:44,630]\u001b[0m Trial 909 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:45,405]\u001b[0m Trial 910 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:45,853]\u001b[0m Trial 911 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:49,408]\u001b[0m Trial 912 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:52,054]\u001b[0m Trial 913 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:53,437]\u001b[0m Trial 914 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:14:55,610]\u001b[0m Trial 915 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:07,906]\u001b[0m Trial 916 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:12,979]\u001b[0m Trial 917 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:18,273]\u001b[0m Trial 918 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:30,616]\u001b[0m Trial 919 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:36,054]\u001b[0m Trial 920 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:36,863]\u001b[0m Trial 921 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:44,597]\u001b[0m Trial 922 finished with value: 0.7571428571428571 and parameters: {'n_epochs': 135, 'batch_size': 6, 'learning_rate': 0.0054860394173045345, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 9, 'n_units_l1': 7, 'n_units_l2': 4, 'n_units_l3': 7}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:47,230]\u001b[0m Trial 923 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:49,240]\u001b[0m Trial 924 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:49,726]\u001b[0m Trial 925 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:50,406]\u001b[0m Trial 926 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:54,638]\u001b[0m Trial 927 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:57,582]\u001b[0m Trial 928 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:15:57,973]\u001b[0m Trial 929 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:14,693]\u001b[0m Trial 930 finished with value: 0.7785714285714286 and parameters: {'n_epochs': 60, 'batch_size': 4, 'learning_rate': 0.031144216285758763, 'optimizer': 'Adam', 'n_layers': 5, 'n_units_l0': 10, 'n_units_l1': 9, 'n_units_l2': 8, 'n_units_l3': 6, 'n_units_l4': 9}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:15,634]\u001b[0m Trial 931 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:16,065]\u001b[0m Trial 932 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:16,549]\u001b[0m Trial 933 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:22,528]\u001b[0m Trial 934 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:23,328]\u001b[0m Trial 935 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:24,384]\u001b[0m Trial 936 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:27,565]\u001b[0m Trial 937 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:29,939]\u001b[0m Trial 938 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:31,051]\u001b[0m Trial 939 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:31,513]\u001b[0m Trial 940 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:34,380]\u001b[0m Trial 941 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:35,293]\u001b[0m Trial 942 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:37,884]\u001b[0m Trial 943 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:40,492]\u001b[0m Trial 944 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:42,080]\u001b[0m Trial 945 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:44,635]\u001b[0m Trial 946 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:45,668]\u001b[0m Trial 947 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:48,350]\u001b[0m Trial 948 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:48,695]\u001b[0m Trial 949 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:51,408]\u001b[0m Trial 950 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:52,298]\u001b[0m Trial 951 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:16:54,431]\u001b[0m Trial 952 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:30,647]\u001b[0m Trial 953 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:31,864]\u001b[0m Trial 954 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:34,130]\u001b[0m Trial 955 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:34,469]\u001b[0m Trial 956 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:36,694]\u001b[0m Trial 957 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:38,916]\u001b[0m Trial 958 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:39,504]\u001b[0m Trial 959 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:17:51,653]\u001b[0m Trial 960 finished with value: 0.7928571428571428 and parameters: {'n_epochs': 69, 'batch_size': 4, 'learning_rate': 0.0003404231853272512, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 4, 'n_units_l1': 10, 'n_units_l2': 6, 'n_units_l3': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:35,253]\u001b[0m Trial 961 finished with value: 0.8 and parameters: {'n_epochs': 96, 'batch_size': 3, 'learning_rate': 0.002172214325878554, 'optimizer': 'Adam', 'n_layers': 4, 'n_units_l0': 8, 'n_units_l1': 8, 'n_units_l2': 7, 'n_units_l3': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:36,175]\u001b[0m Trial 962 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:38,290]\u001b[0m Trial 963 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:55,705]\u001b[0m Trial 964 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:56,790]\u001b[0m Trial 965 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:58,173]\u001b[0m Trial 966 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:18:59,509]\u001b[0m Trial 967 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:00,022]\u001b[0m Trial 968 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:01,562]\u001b[0m Trial 969 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:16,959]\u001b[0m Trial 970 finished with value: 0.7714285714285715 and parameters: {'n_epochs': 97, 'batch_size': 5, 'learning_rate': 0.001008036366002222, 'optimizer': 'Adam', 'n_layers': 6, 'n_units_l0': 7, 'n_units_l1': 8, 'n_units_l2': 4, 'n_units_l3': 6, 'n_units_l4': 6, 'n_units_l5': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:17,238]\u001b[0m Trial 971 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:18,269]\u001b[0m Trial 972 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:19,203]\u001b[0m Trial 973 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:24,182]\u001b[0m Trial 974 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:24,470]\u001b[0m Trial 975 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:26,810]\u001b[0m Trial 976 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:27,744]\u001b[0m Trial 977 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:28,084]\u001b[0m Trial 978 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:29,370]\u001b[0m Trial 979 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:31,433]\u001b[0m Trial 980 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:44,663]\u001b[0m Trial 981 finished with value: 0.8142857142857143 and parameters: {'n_epochs': 115, 'batch_size': 5, 'learning_rate': 0.009888732887255256, 'optimizer': 'RMSprop', 'n_layers': 4, 'n_units_l0': 6, 'n_units_l1': 10, 'n_units_l2': 8, 'n_units_l3': 5}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:45,108]\u001b[0m Trial 982 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:47,063]\u001b[0m Trial 983 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:48,543]\u001b[0m Trial 984 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:53,413]\u001b[0m Trial 985 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:56,078]\u001b[0m Trial 986 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:56,773]\u001b[0m Trial 987 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:19:59,468]\u001b[0m Trial 988 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:01,614]\u001b[0m Trial 989 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:04,380]\u001b[0m Trial 990 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:05,866]\u001b[0m Trial 991 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:09,167]\u001b[0m Trial 992 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:11,292]\u001b[0m Trial 993 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:17,055]\u001b[0m Trial 994 finished with value: 0.7857142857142857 and parameters: {'n_epochs': 39, 'batch_size': 4, 'learning_rate': 0.014915017818690323, 'optimizer': 'RMSprop', 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 6}. Best is trial 185 with value: 0.8785714285714286.\u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:17,573]\u001b[0m Trial 995 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:18,128]\u001b[0m Trial 996 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:18,837]\u001b[0m Trial 997 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:23,569]\u001b[0m Trial 998 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-08-31 17:20:26,272]\u001b[0m Trial 999 pruned. \u001b[0m\n",
      "Study report:\n",
      "\t\n",
      "\n",
      "\tn_epoch = 74, batch_size = 2\n",
      "\tAccuracy = 0.8785714285714286\n"
     ]
    }
   ],
   "source": [
    "# initializing some variables\n",
    "models = dict()\n",
    "models['accuracy'] = 0\n",
    "\n",
    "# initializing and calling optimize for each study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(lambda trial: objective(trial), n_trials=1000, show_progress_bar=True)\n",
    "\n",
    "# extracting the best trial of the study\n",
    "best_params = study.best_trial\n",
    "param_dict = best_params.params\n",
    "param_dict['batch_size'] = 2**param_dict['batch_size']\n",
    "# saving details of the trial\n",
    "params_dict = dict()\n",
    "best_model_index = best_params.number\n",
    "params_dict['model'] = models['model']\n",
    "\n",
    "params_dict['params'] = param_dict\n",
    "params_dict['accuracy'] = best_params.value\n",
    "\n",
    "print(f\"Study report:\\n\\t\", end='\\n\\n')\n",
    "print(f\"\\tn_epoch = {param_dict['n_epochs']}, batch_size = {param_dict['batch_size']}\")\n",
    "print(f\"\\tAccuracy = {params_dict['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model:\n",
      "\tAccuracy: 0.8785714285714286\n",
      "\tModel Saved Path: 'Models\\customer_loyalty_prediction_auto.pt'\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': params_dict['model'].state_dict(),\n",
    "    'accuracy': params_dict['accuracy'],\n",
    "    'architecture': params_dict['params'] \n",
    "    }, 'Models\\\\customer_loyalty_prediction_auto.pt')\n",
    "\n",
    "print(f\"Final model:\\n\\t\"\n",
    "    + f\"Accuracy: {params_dict['accuracy']}\\n\\t\"\n",
    "    + f\"Model Saved Path: 'Models\\\\customer_loyalty_prediction_auto.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('CLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fd8e93d66050a205cb7f64389af9f13c9c81ade6ed89497904bd8d3e012a9c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
